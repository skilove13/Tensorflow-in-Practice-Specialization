{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF자격증_풀잎스쿨.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skilove13/Tensorflow-in-Practice-Specialization/blob/master/TF%EC%9E%90%EA%B2%A9%EC%A6%9D_%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bMzvWxTDp8p",
        "colab_type": "text"
      },
      "source": [
        "# **1. 텐서플로우 기초** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM-_n31Y7Ngx",
        "colab_type": "text"
      },
      "source": [
        "## TF_1-Question\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LukvqiBb7Fwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Getting Started Question\n",
        "#\n",
        "# Given this data, train a neural network to match the xs to the ys\n",
        "# So that a predictor for a new value of X will give a float value\n",
        "# very close to the desired answer\n",
        "# i.e. print(model.predict([10.0])) would give a satisfactory result\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "    ys = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    model\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vImSup7OyXcN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d3839c6-fb22-4988-e92c-9e824eb0a935"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "def solution_model():\n",
        "  xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0],  dtype=float)\n",
        "  ys = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=float)\n",
        "\n",
        "  model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "  model.compile(optimizer = 'sgd', loss='mean_squared_error')\n",
        "\n",
        "  model.fit(xs, ys, epochs=250)\n",
        "\n",
        "  return model\n",
        "\n",
        "model = solution_model()\n",
        "model.save('number1.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 85.6491\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 70.0264\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 57.6810\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 47.9150\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 40.1796\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 34.0427\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 29.1646\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 25.2778\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 22.1719\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 19.6815\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 17.6761\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 16.0534\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 14.7326\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 13.6503\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 12.7565\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 12.0119\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 11.3854\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 10.8529\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 10.3949\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.9965\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.6457\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.3331\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0513\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.7946\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5582\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3386\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.1328\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 0s 962us/step - loss: 7.9386\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7542\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.5781\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.4092\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.2466\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.0896\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.9375\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.7900\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.6465\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.5069\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.3707\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.2379\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1083\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9816\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.8578\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7367\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.6183\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.5024\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.3891\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2781\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1694\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0631\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9589\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8570\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7571\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6593\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5636\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4698\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3780\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2880\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1999\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1136\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.0291\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.9463\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8653\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7859\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7081\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.6319\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5573\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4843\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4127\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.3426\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2739\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2067\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1408\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.0763\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0131\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9512\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8906\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8312\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7731\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.7161\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.6603\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6057\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5521\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4997\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4484\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3981\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.3488\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.3006\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2533\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2070\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.1617\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.1173\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.0738\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.0312\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9895\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9486\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9086\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8694\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8310\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.7934\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7566\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7205\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.6851\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6505\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.6166\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5834\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5509\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5190\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4878\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4573\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4273\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3980\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.3693\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3412\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3136\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2866\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2602\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2343\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2090\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1841\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1598\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1360\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1127\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0898\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0674\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0455\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0240\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0030\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9824\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.9622\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9424\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.9231\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.9041\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8856\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.8674\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8495\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8321\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8150\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7983\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7819\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7658\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7501\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7347\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7196\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.7048\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6903\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6761\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6623\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6486\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6353\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6223\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6095\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5970\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5847\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5727\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5609\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5494\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5381\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5271\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5163\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5056\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4953\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4851\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4751\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4654\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4558\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4464\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4373\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4283\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4195\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4109\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4024\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3942\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3861\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3781\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3704\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3628\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3553\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3480\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3409\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3339\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3270\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3203\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3137\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3073\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3010\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2948\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2887\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2828\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2770\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2713\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2657\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2603\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2549\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2497\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2446\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2395\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2346\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2298\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2251\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2204\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2159\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2115\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2071\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2029\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1987\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1946\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1906\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1867\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1829\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1791\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1755\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1718\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1683\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1649\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1615\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1582\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1549\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1517\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1486\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1456\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1426\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1396\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1368\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1340\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1312\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1285\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1259\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1233\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1208\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1183\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1158\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1135\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1111\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1089\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1066\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1044\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1023\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1002\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0981\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0961\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0941\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0922\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0903\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0885\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0866\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0849\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0831\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0814\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0797\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL6s5wXw2p1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32f2a666-21a4-492a-8451-828938a79504"
      },
      "source": [
        "print(model.predict([10.0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[16.815334]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeZzw4JeEsMU",
        "colab_type": "text"
      },
      "source": [
        "## **코드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0BIg3lBEQ0i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06da699c-30f1-4c97-a457-1175aa099a75"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=5)\n",
        "\n",
        "print(model.predict([10.0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0183\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5307\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1438\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8363\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5913\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3956\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2386\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1122\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0100\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9268\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8586\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8023\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7553\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7159\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6824\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6535\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6284\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6064\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5867\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5690\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5528\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5379\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5241\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5112\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4990\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4874\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4763\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4657\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4555\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4456\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4360\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4268\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4177\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4090\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4004\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3921\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3839\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3760\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3682\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3606\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3531\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3458\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3387\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3317\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3249\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3182\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3117\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3053\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2990\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2928\n",
            "[[17.405905]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKMh7KzFE7L4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c902071-26d8-4df6-c41a-beebdab2ff14"
      },
      "source": [
        "print(model.predict([10.0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[16.942162]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQbZZHhZDong",
        "colab_type": "text"
      },
      "source": [
        "# **2. 분류 문제**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQfTxXS-7xyF",
        "colab_type": "text"
      },
      "source": [
        "## TF_2-mnist-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afdEy58g7WmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Basic Datasets Question\n",
        "#\n",
        "# Create a classifier for the MNIST dataset\n",
        "# Note that the test will expect it to classify 10 classes and that the \n",
        "# input shape should be the native size of the MNIST dataset which is \n",
        "# 28x28 monochrome. Do not resize the data. Your input layer should accept\n",
        "# (28,28) as the input shape only. If you amend this, the tests will fail.\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def solution_model():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (training_images, training_labels), (test_imagges, test_labels) = mnist.load_data()\n",
        "\n",
        "    model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "\n",
        "    model = None\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeFWZSNH5NZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "b8d08b36-354b-45b1-977c-787fd6b9a51e"
      },
      "source": [
        "## Ver1\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.6570 - accuracy: 0.8474\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4207 - accuracy: 0.9032\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3072 - accuracy: 0.9237\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2674 - accuracy: 0.9336\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2337 - accuracy: 0.9433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb8d2550b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SUukzRC587t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "21b2bad2-773a-4b7b-9b36-8ef5f33d78cf"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.9392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30510321259498596, 0.9391999840736389]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rNuksJh6bCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a26a291-2a3f-4b81-f118-ed5aa5faf29c"
      },
      "source": [
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWn7VD4sHRA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "5706c241-84d4-4c83-8496-c786064a3fef"
      },
      "source": [
        "## Ver2\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1888\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0807\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0545\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0426\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0344\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0849\n",
            "[1.14465815e-11 6.42732232e-08 1.95475298e-08 3.23165580e-07\n",
            " 3.02663644e-10 5.24422228e-10 7.91309496e-14 9.99998808e-01\n",
            " 4.20764257e-10 7.12154247e-07]\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTZS9iu2EYzl",
        "colab_type": "text"
      },
      "source": [
        "## TF_2 - fashion mnist-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTx0Ct4t8OU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Basic Datasets Question\n",
        "#\n",
        "# Create a classifier for the Fashion MNIST dataset\n",
        "# Note that the test will expect it to classify 10 classes and that the \n",
        "# input shape should be the native size of the Fashion MNIST dataset which is \n",
        "# 28x28 monochrome. Do not resize the data. YOur input layer should accept\n",
        "# (28,28) as the input shape only. If you amend this, the tests will fail.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z96nk059xEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy4NMKaM9x-R",
        "colab_type": "text"
      },
      "source": [
        "## TF_2-iris-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY0hbK3y9XIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Basic Datasets question\n",
        "#\n",
        "# For this task you will train a classifier for Iris flowers using the Iris dataset\n",
        "# The final layer in your neural network should look like: tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
        "# The input layer will expect data in the shape (4,)\n",
        "# We've given you some starter code for preprocessing the data\n",
        "# You'll need to implement the preprocess function for data.map\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "data = tfds.load(\"iris\", split=tfds.Split.TRAIN.subsplit(tfds.percent[:80]))\n",
        "\n",
        "def preprocess(features):\n",
        "    # YOUR CODE HERE\n",
        "    # Should return features and one-hot encoded labels\n",
        "    return f,l\n",
        "\n",
        "def solution_model():\n",
        "    train_dataset = data.map(preprocess).batch(10)\n",
        "\n",
        "    # YOUR CODE TO TRAIN A MODEL HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNG_8y72-zpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data, train_data = tfds.load(\n",
        "    name=\"iris\", \n",
        "    split=('train[:20%]', 'train[20%:]'),\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IIJIduh7MEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "852c29d1-5b1c-4339-c4cb-ea7849761a9a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "test_data, train_data = tfds.load(\n",
        "    name=\"iris\", \n",
        "    split=('train[:20%]', 'train[20%:]'),\n",
        "    as_supervised=False)\n",
        "\n",
        "train_data_len = len(list(train_data.as_numpy_iterator()))\n",
        "test_data_len = len(list(test_data.as_numpy_iterator()))\n",
        "\n",
        "def preprocess(dataset):\n",
        "    features = tf.unstack(dataset['features'])\n",
        "    labels = dataset['label']\n",
        "\n",
        "    # f = dict(zip(col_names, features))\n",
        "    f = features\n",
        "    l = tf.one_hot(labels, 3)\n",
        "    return f, l\n",
        "\n",
        "def solution_model():\n",
        "    train_examples_batch, train_labels_batch = next(iter(train_data.map(preprocess).batch(train_data_len)))\n",
        "    test_examples_batch, test_labels_batch = next(iter(test_data.map(preprocess).batch(test_data_len)))\n",
        "\n",
        "    # 모델 설계\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(32, activation='relu'))\n",
        "    model.add(keras.layers.Dense(16, activation='relu'))\n",
        "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "    # 모델 컴파일\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    # 훈련 체크포인트 설계\n",
        "    checkpoint_path = 'iris_chkpnt.ckpt'\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                    save_weights_only=True, \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_loss',\n",
        "                                                    verbose=1)\n",
        "    \n",
        "    # 모델 학습\n",
        "    model.fit(train_examples_batch, train_labels_batch, \n",
        "              validation_data=(test_examples_batch, test_labels_batch),\n",
        "              steps_per_epoch=train_data_len,\n",
        "              validation_steps=test_data_len,\n",
        "              callbacks=[checkpoint],\n",
        "              epochs=50)\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    # 모델 평가\n",
        "    results = model.evaluate(test_examples_batch, test_labels_batch, verbose=2)\n",
        "    print(\"test loss, test acc:\", results)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"irisModel.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 90/120 [=====================>........] - ETA: 0s - loss: 1.3871 - accuracy: 0.4778\n",
            "Epoch 00001: val_loss improved from inf to 0.76355, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 1.2660 - accuracy: 0.5250 - val_loss: 0.7636 - val_accuracy: 0.7333\n",
            "Epoch 2/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.7593 - accuracy: 0.6237\n",
            "Epoch 00002: val_loss improved from 0.76355 to 0.55356, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.7174 - accuracy: 0.6667 - val_loss: 0.5536 - val_accuracy: 0.8667\n",
            "Epoch 3/50\n",
            " 82/120 [===================>..........] - ETA: 0s - loss: 0.5799 - accuracy: 0.7439\n",
            "Epoch 00003: val_loss improved from 0.55356 to 0.43982, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.8083 - val_loss: 0.4398 - val_accuracy: 1.0000\n",
            "Epoch 4/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.4796 - accuracy: 0.8925\n",
            "Epoch 00004: val_loss improved from 0.43982 to 0.35788, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.9000 - val_loss: 0.3579 - val_accuracy: 0.9333\n",
            "Epoch 5/50\n",
            " 95/120 [======================>.......] - ETA: 0s - loss: 0.3916 - accuracy: 0.8842\n",
            "Epoch 00005: val_loss improved from 0.35788 to 0.30325, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8833 - val_loss: 0.3032 - val_accuracy: 0.8667\n",
            "Epoch 6/50\n",
            "118/120 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.9407\n",
            "Epoch 00006: val_loss improved from 0.30325 to 0.27049, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.9417 - val_loss: 0.2705 - val_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            " 97/120 [=======================>......] - ETA: 0s - loss: 0.3259 - accuracy: 0.9175\n",
            "Epoch 00007: val_loss improved from 0.27049 to 0.27004, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.9250 - val_loss: 0.2700 - val_accuracy: 0.9667\n",
            "Epoch 8/50\n",
            "116/120 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9052\n",
            "Epoch 00008: val_loss improved from 0.27004 to 0.22948, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.9083 - val_loss: 0.2295 - val_accuracy: 0.9667\n",
            "Epoch 9/50\n",
            " 97/120 [=======================>......] - ETA: 0s - loss: 0.2555 - accuracy: 0.9588\n",
            "Epoch 00009: val_loss did not improve from 0.22948\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.9667 - val_loss: 0.2573 - val_accuracy: 0.9000\n",
            "Epoch 10/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.2458 - accuracy: 0.9247\n",
            "Epoch 00010: val_loss improved from 0.22948 to 0.20522, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.2392 - accuracy: 0.9333 - val_loss: 0.2052 - val_accuracy: 0.9667\n",
            "Epoch 11/50\n",
            " 97/120 [=======================>......] - ETA: 0s - loss: 0.2385 - accuracy: 0.9175\n",
            "Epoch 00011: val_loss improved from 0.20522 to 0.18768, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9333 - val_loss: 0.1877 - val_accuracy: 0.9667\n",
            "Epoch 12/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.2142 - accuracy: 0.9570\n",
            "Epoch 00012: val_loss improved from 0.18768 to 0.16900, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9583 - val_loss: 0.1690 - val_accuracy: 0.9667\n",
            "Epoch 13/50\n",
            " 82/120 [===================>..........] - ETA: 0s - loss: 0.1759 - accuracy: 0.9390\n",
            "Epoch 00013: val_loss improved from 0.16900 to 0.12639, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9583 - val_loss: 0.1264 - val_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            " 94/120 [======================>.......] - ETA: 0s - loss: 0.1843 - accuracy: 0.9362\n",
            "Epoch 00014: val_loss did not improve from 0.12639\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9500 - val_loss: 0.1344 - val_accuracy: 0.9333\n",
            "Epoch 15/50\n",
            " 81/120 [===================>..........] - ETA: 0s - loss: 0.1503 - accuracy: 0.9506\n",
            "Epoch 00015: val_loss improved from 0.12639 to 0.12229, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1668 - accuracy: 0.9417 - val_loss: 0.1223 - val_accuracy: 0.9667\n",
            "Epoch 16/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.1450 - accuracy: 0.9677\n",
            "Epoch 00016: val_loss did not improve from 0.12229\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1536 - accuracy: 0.9667 - val_loss: 0.1505 - val_accuracy: 0.9667\n",
            "Epoch 17/50\n",
            " 94/120 [======================>.......] - ETA: 0s - loss: 0.1669 - accuracy: 0.9362\n",
            "Epoch 00017: val_loss improved from 0.12229 to 0.08952, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9500 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            " 86/120 [====================>.........] - ETA: 0s - loss: 0.1350 - accuracy: 0.9535\n",
            "Epoch 00018: val_loss did not improve from 0.08952\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9583 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
            "Epoch 19/50\n",
            " 90/120 [=====================>........] - ETA: 0s - loss: 0.1543 - accuracy: 0.9333\n",
            "Epoch 00019: val_loss improved from 0.08952 to 0.07540, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9500 - val_loss: 0.0754 - val_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            " 92/120 [======================>.......] - ETA: 0s - loss: 0.1224 - accuracy: 0.9457\n",
            "Epoch 00020: val_loss improved from 0.07540 to 0.07162, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9500 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.0932 - accuracy: 0.9892\n",
            "Epoch 00021: val_loss did not improve from 0.07162\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1188 - accuracy: 0.9750 - val_loss: 0.0803 - val_accuracy: 0.9667\n",
            "Epoch 22/50\n",
            " 93/120 [======================>.......] - ETA: 0s - loss: 0.1236 - accuracy: 0.9677\n",
            "Epoch 00022: val_loss improved from 0.07162 to 0.06853, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1199 - accuracy: 0.9667 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "Epoch 23/50\n",
            " 84/120 [====================>.........] - ETA: 0s - loss: 0.1142 - accuracy: 0.9405\n",
            "Epoch 00023: val_loss did not improve from 0.06853\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.9500 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
            "Epoch 24/50\n",
            " 89/120 [=====================>........] - ETA: 0s - loss: 0.1450 - accuracy: 0.9438\n",
            "Epoch 00024: val_loss improved from 0.06853 to 0.05608, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9583 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            " 81/120 [===================>..........] - ETA: 0s - loss: 0.1062 - accuracy: 0.9630\n",
            "Epoch 00025: val_loss did not improve from 0.05608\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1124 - accuracy: 0.9583 - val_loss: 0.0746 - val_accuracy: 0.9667\n",
            "Epoch 26/50\n",
            " 92/120 [======================>.......] - ETA: 0s - loss: 0.1237 - accuracy: 0.9457\n",
            "Epoch 00026: val_loss improved from 0.05608 to 0.05498, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            " 89/120 [=====================>........] - ETA: 0s - loss: 0.1388 - accuracy: 0.9213\n",
            "Epoch 00027: val_loss did not improve from 0.05498\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1104 - accuracy: 0.9417 - val_loss: 0.0596 - val_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            " 82/120 [===================>..........] - ETA: 0s - loss: 0.1486 - accuracy: 0.9634\n",
            "Epoch 00028: val_loss improved from 0.05498 to 0.04715, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.9750 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            " 91/120 [=====================>........] - ETA: 0s - loss: 0.1268 - accuracy: 0.9451\n",
            "Epoch 00029: val_loss did not improve from 0.04715\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
            "Epoch 30/50\n",
            " 92/120 [======================>.......] - ETA: 0s - loss: 0.0987 - accuracy: 0.9674\n",
            "Epoch 00030: val_loss did not improve from 0.04715\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9667 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            " 96/120 [=======================>......] - ETA: 0s - loss: 0.1315 - accuracy: 0.9375\n",
            "Epoch 00031: val_loss improved from 0.04715 to 0.04037, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1079 - accuracy: 0.9500 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "117/120 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9829\n",
            "Epoch 00032: val_loss did not improve from 0.04037\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.9833 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            " 85/120 [====================>.........] - ETA: 0s - loss: 0.1288 - accuracy: 0.9647\n",
            "Epoch 00033: val_loss did not improve from 0.04037\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9667 - val_loss: 0.0487 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            " 94/120 [======================>.......] - ETA: 0s - loss: 0.0915 - accuracy: 0.9787\n",
            "Epoch 00034: val_loss improved from 0.04037 to 0.03397, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9750 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            " 88/120 [=====================>........] - ETA: 0s - loss: 0.1352 - accuracy: 0.9432\n",
            "Epoch 00035: val_loss improved from 0.03397 to 0.03261, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9583 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            " 89/120 [=====================>........] - ETA: 0s - loss: 0.1025 - accuracy: 0.9888\n",
            "Epoch 00036: val_loss improved from 0.03261 to 0.03176, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9833 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            " 76/120 [==================>...........] - ETA: 0s - loss: 0.1188 - accuracy: 0.9211\n",
            "Epoch 00037: val_loss did not improve from 0.03176\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9417 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            " 91/120 [=====================>........] - ETA: 0s - loss: 0.1127 - accuracy: 0.9670\n",
            "Epoch 00038: val_loss did not improve from 0.03176\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.9750 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "111/120 [==========================>...] - ETA: 0s - loss: 0.1120 - accuracy: 0.9550\n",
            "Epoch 00039: val_loss did not improve from 0.03176\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1081 - accuracy: 0.9583 - val_loss: 0.0745 - val_accuracy: 0.9667\n",
            "Epoch 40/50\n",
            " 88/120 [=====================>........] - ETA: 0s - loss: 0.0867 - accuracy: 0.9545\n",
            "Epoch 00040: val_loss did not improve from 0.03176\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1010 - accuracy: 0.9500 - val_loss: 0.0687 - val_accuracy: 0.9667\n",
            "Epoch 41/50\n",
            " 88/120 [=====================>........] - ETA: 0s - loss: 0.1199 - accuracy: 0.9432\n",
            "Epoch 00041: val_loss improved from 0.03176 to 0.02697, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9583 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            " 84/120 [====================>.........] - ETA: 0s - loss: 0.0987 - accuracy: 0.9524\n",
            "Epoch 00042: val_loss did not improve from 0.02697\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1096 - accuracy: 0.9500 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "110/120 [==========================>...] - ETA: 0s - loss: 0.0960 - accuracy: 0.9636\n",
            "Epoch 00043: val_loss did not improve from 0.02697\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9667 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            " 81/120 [===================>..........] - ETA: 0s - loss: 0.0926 - accuracy: 0.9753\n",
            "Epoch 00044: val_loss did not improve from 0.02697\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9667 - val_loss: 0.0847 - val_accuracy: 0.9667\n",
            "Epoch 45/50\n",
            "114/120 [===========================>..] - ETA: 0s - loss: 0.1034 - accuracy: 0.9474\n",
            "Epoch 00045: val_loss did not improve from 0.02697\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9500 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
            "Epoch 46/50\n",
            "114/120 [===========================>..] - ETA: 0s - loss: 0.1014 - accuracy: 0.9561\n",
            "Epoch 00046: val_loss did not improve from 0.02697\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.9583 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
            "Epoch 47/50\n",
            " 77/120 [==================>...........] - ETA: 0s - loss: 0.0809 - accuracy: 0.9610\n",
            "Epoch 00047: val_loss did not improve from 0.02697\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9583 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "117/120 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9487\n",
            "Epoch 00048: val_loss improved from 0.02697 to 0.02275, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9500 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "118/120 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9831\n",
            "Epoch 00049: val_loss did not improve from 0.02275\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.0984 - accuracy: 0.9833 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            " 88/120 [=====================>........] - ETA: 0s - loss: 0.0456 - accuracy: 0.9886\n",
            "Epoch 00050: val_loss improved from 0.02275 to 0.02047, saving model to iris_chkpnt.ckpt\n",
            "120/120 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9667 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "1/1 - 0s - loss: 0.0205 - accuracy: 1.0000\n",
            "test loss, test acc: [0.020472245290875435, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBZUcYPjEmYG",
        "colab_type": "text"
      },
      "source": [
        "# **3. 분류문제 - file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJx1AYifACo4",
        "colab_type": "text"
      },
      "source": [
        "## TF_3-rps-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUZHe7h6ADWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "\n",
        "# Computer Vision with CNNs\n",
        "\n",
        "# For this task you will build a classifier for Rock-Paper-Scissors \n",
        "# based on the rps dataset.\n",
        "\n",
        "# IMPORTANT: Your final layer should be as shown, do not change the\n",
        "# provided code, or the tests may fail\n",
        "\n",
        "# IMPORTANT: Images will be tested as 150x150 with 3 bytes of color depth\n",
        "# So ensure that your input layer is designed accordingly, or the tests\n",
        "# may fail. \n",
        "\n",
        "# NOTE THAT THIS IS UNLABELLED DATA. \n",
        "# You can use the ImageDataGenerator to automatically label it\n",
        "# and we have provided some starter code.\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
        "    urllib.request.urlretrieve(url, 'rps.zip')\n",
        "    local_zip = 'rps.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/')\n",
        "    zip_ref.close()\n",
        "\n",
        "\n",
        "    TRAINING_DIR = \"tmp/rps/\"\n",
        "    training_datagen = ImageDataGenerator(\n",
        "    # YOUR CODE HERE)\n",
        "\n",
        "    train_generator = # YOUR CODE HERE\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "    # YOUR CODE HERE, BUT END WITH A 3 Neuron Dense, activated by softmax\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z7waYDGFzXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "ac76598c-3789-42f5-90b9-c0f361edcec6"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
        "    urllib.request.urlretrieve(url, 'rps.zip')\n",
        "    local_zip = 'rps.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = \"tmp/rps/\"\n",
        "    training_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        validation_split=0.2,\n",
        "        rotation_range=20,      # 랜덤하게 사진을 회전시킬 각도 범위(1-180도)\n",
        "        width_shift_range=0.1,  # 사진을 수평으로 랜덤하게 평행이동시킬 범위\n",
        "        height_shift_range=0.1, # 사진을 수직으로 랜덤하게 평행이동시킬 범위\n",
        "        shear_range=0.1,        # 랜덤하게 전단변환을 적용할 각도 범위\n",
        "        zoom_range=0.1,         # 랜덤하게 사진을 확대할 범위\n",
        "        horizontal_flip=True,   # 랜덤하게 이미지를 수평으로 뒤집을지 여부\n",
        "        fill_mode='nearest',    # 회전/이동 등으로 새롭게 생성되는 픽셀을 채울 방법\n",
        "    )\n",
        "\n",
        "    train_generator = training_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                          target_size=(150, 150), \n",
        "                                                          batch_size=20, \n",
        "                                                          class_mode='categorical', \n",
        "                                                          subset='training',\n",
        "                                                         )\n",
        "\n",
        "    validation_generator = training_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                              target_size=(150, 150), \n",
        "                                                              batch_size=20, \n",
        "                                                              class_mode='categorical',\n",
        "                                                              subset='validation',\n",
        "                                                            )\n",
        "    \n",
        "    for data_batch, label_batch in train_generator:\n",
        "      print('배치데이터 크기', data_batch.shape)\n",
        "      print('배치레이블 크기', label_batch.shape)\n",
        "      break\n",
        "\n",
        "    print('train generator 크기', len(train_generator))\n",
        "    print('validation generator 크기', len(validation_generator))\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)),\n",
        "        MaxPooling2D(2, 2), \n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2), \n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2), \n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2), \n",
        "        Flatten(), \n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(3, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "    mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "    model.fit(\n",
        "        train_generator, \n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=30,\n",
        "        validation_data=(validation_generator),\n",
        "        validation_steps=len(validation_generator),\n",
        "        callbacks=[es, mc],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2016 images belonging to 3 classes.\n",
            "Found 504 images belonging to 3 classes.\n",
            "배치데이터 크기 (20, 150, 150, 3)\n",
            "배치레이블 크기 (20, 3)\n",
            "train generator 크기 101\n",
            "validation generator 크기 26\n",
            "Epoch 1/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.9605 - acc: 0.5208\n",
            "Epoch 00001: val_acc improved from -inf to 0.78770, saving model to best_model.h5\n",
            "101/101 [==============================] - 115s 1s/step - loss: 0.9605 - acc: 0.5208 - val_loss: 0.7284 - val_acc: 0.7877\n",
            "Epoch 2/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.4783 - acc: 0.8294\n",
            "Epoch 00002: val_acc improved from 0.78770 to 0.87698, saving model to best_model.h5\n",
            "101/101 [==============================] - 112s 1s/step - loss: 0.4783 - acc: 0.8294 - val_loss: 0.4568 - val_acc: 0.8770\n",
            "Epoch 3/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.2694 - acc: 0.9127\n",
            "Epoch 00003: val_acc did not improve from 0.87698\n",
            "101/101 [==============================] - 112s 1s/step - loss: 0.2694 - acc: 0.9127 - val_loss: 0.4139 - val_acc: 0.8532\n",
            "Epoch 4/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.9400\n",
            "Epoch 00004: val_acc did not improve from 0.87698\n",
            "101/101 [==============================] - 112s 1s/step - loss: 0.1800 - acc: 0.9400 - val_loss: 0.7616 - val_acc: 0.7619\n",
            "Epoch 5/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.1418 - acc: 0.9578\n",
            "Epoch 00005: val_acc did not improve from 0.87698\n",
            "101/101 [==============================] - 112s 1s/step - loss: 0.1418 - acc: 0.9578 - val_loss: 0.6091 - val_acc: 0.8274\n",
            "Epoch 6/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.1152 - acc: 0.9623\n",
            "Epoch 00006: val_acc did not improve from 0.87698\n",
            "101/101 [==============================] - 116s 1s/step - loss: 0.1152 - acc: 0.9623 - val_loss: 0.5155 - val_acc: 0.8651\n",
            "Epoch 7/30\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.1001 - acc: 0.9692\n",
            "Epoch 00007: val_acc did not improve from 0.87698\n",
            "101/101 [==============================] - 112s 1s/step - loss: 0.1001 - acc: 0.9692 - val_loss: 0.6364 - val_acc: 0.8373\n",
            "Epoch 8/30\n",
            " 71/101 [====================>.........] - ETA: 30s - loss: 0.1284 - acc: 0.9703"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaM3hH7AXA6",
        "colab_type": "text"
      },
      "source": [
        "## TF_3-cat-vs-dogs-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcNL1-Z3AX5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Computer Vision with CNNs\n",
        "#\n",
        "# For this exercise you will build a cats v dogs classifier\n",
        "# using the Cats v Dogs dataset from TFDS.\n",
        "# Be sure to use the final layer as shown \n",
        "#     (Dense, 2 neurons, softmax activation)\n",
        "#\n",
        "# The testing infrastructre will resize all images to 224x224 \n",
        "# with 3 bytes of color depth. Make sure your input layer trains\n",
        "# images to that specification, or the tests will fail.\n",
        "#\n",
        "# Make sure your output layer is exactly as specified here, or the \n",
        "# tests will fail.\n",
        "\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "dataset_name = 'cats_vs_dogs'\n",
        "dataset, info = tfds.load(name=dataset_name, split=tfds.Split.TRAIN, with_info=True)\n",
        "\n",
        "def preprocess(features):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "def solution_model():\n",
        "    train_dataset = dataset.map(preprocess).batch(32)\n",
        "\n",
        "    model = # YOUR CODE HERE, BUT MAKE SURE YOUR LAST LAYER HAS 2 NEURONS ACTIVATED BY SOFTMAX\n",
        "        tf.keras.layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17QUE988Ahdd",
        "colab_type": "text"
      },
      "source": [
        "## TF_3-cat-vs-dogs-question B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iAPiGURAiRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Computer Vision with CNNs\n",
        "#\n",
        "# For this exercise you will build a cats v dogs classifier\n",
        "# using the Cats v Dogs dataset from TFDS.\n",
        "# Be sure to use the final layer as shown \n",
        "#     (Dense, 2 neurons, softmax activation)\n",
        "#\n",
        "# The testing infrastructre will resize all images to 224x224 \n",
        "# with 3 bytes of color depth. Make sure your input layer trains\n",
        "# images to that specification, or the tests will fail.\n",
        "#\n",
        "# Make sure your output layer is exactly as specified here, or the \n",
        "# tests will fail.\n",
        "\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "dataset_name = 'cats_vs_dogs'\n",
        "dataset, info = tfds.load(name=dataset_name, split=tfds.Split.TRAIN, with_info=True)\n",
        "\n",
        "def preprocess(features):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "def solution_model():\n",
        "    train_dataset = dataset.map(preprocess).batch(32)\n",
        "\n",
        "    model = # YOUR CODE HERE, BUT MAKE SURE YOUR LAST LAYER HAS 2 NEURONS ACTIVATED BY SOFTMAX\n",
        "        tf.keras.layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PV-UkjfA7Qw",
        "colab_type": "text"
      },
      "source": [
        "## TF_3-horses-or-humans-type-B-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUpwAxILAru8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Question\n",
        "#\n",
        "# This task requires you to create a classifier for horses or humans using\n",
        "# the provided data. Please make sure your final layer is a 1 neuron, activated by sigmoid as shown.\n",
        "# Please note that the test will use images that are 300x300 with 3 bytes color depth so be sure to design your neural network accordingly\n",
        "\n",
        "import tensorflow as tf\n",
        "import urllib\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def solution_model():\n",
        "    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
        "    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
        "    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\n",
        "    local_zip = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/horse-or-human/')\n",
        "    zip_ref.close()\n",
        "    urllib.request.urlretrieve(_TEST_URL, 'validation-horse-or-human.zip')\n",
        "    local_zip = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/validation-horse-or-human/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        #Your code here. Should at least have a rescale. Other parameters can help with overfitting.)\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(#Your Code here)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        #Your Code Here)\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        #Your Code Here)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Note the input shape specified on your first layer must be (300,300,3)\n",
        "        # Your Code here\n",
        "\n",
        "        # This is the last layer. You should not change this code.\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "    model.compile(#Your Code Here#)\n",
        "\n",
        "    model.fit(#Your Code Here#)\n",
        "\n",
        "    # NOTE: If training is taking a very long time, you should consider setting the batch size appropriately on the generator, and the steps per epoch in the model.fit#\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4iV5V--Fch1",
        "colab_type": "text"
      },
      "source": [
        "# **4. NLP Basic**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTccPDrmBMRI",
        "colab_type": "text"
      },
      "source": [
        "## TF_4-sarcasm-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh34bXjABNu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "\tsentences = []\n",
        "\tlabels = []\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSyXrH8Fp_A",
        "colab_type": "text"
      },
      "source": [
        "# **5. 시계열 데이터**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHPhJMUICz8S",
        "colab_type": "text"
      },
      "source": [
        "## TF_5-sunspots-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JrRTQpiC0pF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "027d34c3-7a10-4145-9cc7-1a111b24b51d"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# Sequence Modelling Question.\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv\n",
        "# provided. Your neural network is expected to have an MAE\n",
        "# of at least 20, with top marks going to one with an MAE\n",
        "# of around 15. At the bottom is provided some testing\n",
        "# code should you want to check before uploading which measures\n",
        "# the MAE for you. Strongly recommend you test your model with\n",
        "# this to be able to see how it performs.\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "\t# Your data should be loaded into 2 Python lists called time_step\n",
        "\t# and sunspots. They are decleared here.\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(# YOUR CODE HERE)\n",
        "        time_step.append(# YOUR CODE HERE)\n",
        "\n",
        "\t# You should use numpy to create \n",
        "\t# - your series from the list of sunspots\n",
        "\t# - your time details from the list of time steps\n",
        "    series = # YOUR CODE HERE\n",
        "    time = np.array(time_step)\n",
        "\n",
        "\t# You should split the dataset into training and validation splits\n",
        "\t# At time 3000. So everything up to 3000 is training, and everything\n",
        "\t# after 3000 is validation. Write the code below to achieve that.\n",
        "    split_time = 3000\n",
        "    time_train = # YOUR CODE HERE\n",
        "    x_train = # YOUR CODE HERE\n",
        "    time_valid = # YOUR CODE HERE\n",
        "    x_valid = # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    # You can use any random seed you want. We use 51. :)\n",
        "    tf.random.set_seed(51)\n",
        "    np.random.seed(51)\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "    \n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "      # YOUR CODE HERE. DO NOT CHANGE THE FINAL TWO LAYERS FROM BELOW\n",
        "      tf.keras.layers.Dense(1),\n",
        "      # The data is not normalized, so this lambda layer helps\n",
        "      # keep the MAE in line with expectations. Do not modify.\n",
        "      tf.keras.layers.Lambda(lambda x: x * 400)\n",
        "    ])\n",
        "\n",
        "\n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n",
        "\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "#def model_forecast(model, series, window_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(32).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "\n",
        "#window_size = # YOUR CODE HERE\n",
        "#rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "#rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
        "\n",
        "#result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "\n",
        "## WE EXPECT AN MAE OF 15 or less for the maximum score\n",
        "#score = ceil(20 - result)\n",
        "#if score > 5:\n",
        "#    score = 5\n",
        "\n",
        "#print(score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1a671f6d88bc>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    time = np.array(time_step)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdi7mwo9DIYJ",
        "colab_type": "text"
      },
      "source": [
        "## TF_5-sunspots-2-question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoLhEOZGDJjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# QUESTION\n",
        "#\n",
        "# For this task you will need to train a neural network\n",
        "# to predict sunspot activity using the Sunspots.csv dataset.\n",
        "# Your neural network must  have an MAE\n",
        "# of 0.12 or less on the normalized dataset for top marks.\n",
        "# Code for normalizing the data is provided and should not be changed.\n",
        "# At the bottom of this file, we provide  some testing\n",
        "# code should you want to check your model.\n",
        "\n",
        "# Note: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure.\n",
        "\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(# YOUR CODE HERE)\n",
        "        time_step.append(# YOUR CODE HERE)\n",
        "\n",
        "    series = # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # This is the normalization function\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # The data should be split into training and validation sets at time step 3000\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 3000\n",
        "\n",
        "\n",
        "    time_train = # YOUR CODE HERE\n",
        "    x_train = # YOUR CODE HERE\n",
        "    time_valid = # YOUR CODE HERE\n",
        "    x_valid = # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "      # YOUR CODE HERE. Whatever your first layer is, the input shape will be [None,1] when using the Windowed_dataset above, depending on the layer type chosen\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    # PLEASE NOTE IF YOU SEE THIS TEXT WHILE TRAINING -- IT IS SAFE TO IGNORE\n",
        "    # BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
        "    # \t [[{{node IteratorGetNext}}]]\n",
        "    #\n",
        "\n",
        "\n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, this .h5 model will be\n",
        "# sent to the testing infrastructure for scoring.\n",
        "\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in each category before you finally submit your exam.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n",
        "\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "#def model_forecast(model, series, window_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(32).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "\n",
        "\n",
        "#window_size = # YOUR CODE HERE\n",
        "#rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "#rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
        "\n",
        "#result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "\n",
        "## To get the maximum score, your model must have an MAE OF .12 or less.\n",
        "## When you Submit and Test your model, the grading infrastructure\n",
        "## converts the MAE of your model to a score from 0 to 5 as follows:\n",
        "\n",
        "#test_val = 100 * result\n",
        "#score = math.ceil(17 - test_val)\n",
        "#if score > 5:\n",
        "#    score = 5\n",
        "\n",
        "#print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa7T8XtSyuH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}